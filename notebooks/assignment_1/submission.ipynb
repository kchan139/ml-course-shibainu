{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Notebook\n",
    "\n",
    "This notebook loads preprocessed text data and evaluates several machine learning models:\n",
    "- Hidden Markov Model\n",
    "- Naive Bayes\n",
    "- Neural Network\n",
    "- Bayesian Network\n",
    "- Decision Tree\n",
    "\n",
    "Each model is trained and evaluated with accuracy scores, classification reports, and confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "d:\\Documents\\CODE\\HCMUT\\Machine Learning Assignment\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from src.data.preprocess import DataPreprocessor\n",
    "from src.models.train_model import ModelTrainer\n",
    "from src.models.predict_model import ModelPredictor\n",
    "from src.config import *\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Loading the data, preprocessing it, and vectorizing the text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = DataPreprocessor(TEST_DIR)\n",
    "preprocessor.clean_data()\n",
    "X_train, X_test, y_train, y_test = preprocessor.split_data(test_size=0.2)\n",
    "(X_train_vec, X_test_vec), vectorizer = preprocessor.vectorize_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Helper\n",
    "\n",
    "Define a function to evaluate model performance using standard metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"\\n=== {model_name} Performance ===\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and Evaluation\n",
    "\n",
    "Initialize the trainer and predictor objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelTrainer()\n",
    "predictor = ModelPredictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Hidden Markov Model\n",
    "\n",
    "Applying dimensionality reduction before training the HMM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully trained HMM for sentiment 0 with 2400 samples\n",
      "Successfully trained HMM for sentiment 1 with 2471 samples\n",
      "Successfully trained HMM for sentiment 2 with 2399 samples\n",
      "HMM models saved at: d:\\Documents\\CODE\\HCMUT\\Machine Learning Assignment\\models\\trained\\hmm_models.pkl\n",
      "\n",
      "=== Hidden Markov Model Performance ===\n",
      "Accuracy: 0.3273\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.02      0.04       600\n",
      "           1       0.00      0.00      0.00       618\n",
      "           2       0.33      0.97      0.49       600\n",
      "\n",
      "    accuracy                           0.33      1818\n",
      "   macro avg       0.25      0.33      0.18      1818\n",
      "weighted avg       0.25      0.33      0.18      1818\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 14   0 586]\n",
      " [  0   0 618]\n",
      " [ 19   0 581]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\CODE\\HCMUT\\Machine Learning Assignment\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Documents\\CODE\\HCMUT\\Machine Learning Assignment\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Documents\\CODE\\HCMUT\\Machine Learning Assignment\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality reduction for HMM\n",
    "svd = TruncatedSVD(n_components=40)\n",
    "X_train_hmm = svd.fit_transform(X_train_vec)\n",
    "X_test_hmm = svd.transform(X_test_vec)\n",
    "hmm_model = trainer.train_hidden_markov_model(X_train_hmm, y_train, n_components=2)\n",
    "hmm_pred = predictor.predict_hidden_markov_model(X_test_hmm, trainer)\n",
    "evaluate_model(y_test, hmm_pred, \"Hidden Markov Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the Hidden Markov Model (HMM), the accuracy is 32.7%, which is quite low. The confusion matrix shows it mostly misclassifies classes 0 and 2. HMMs are typically used for sequential data, so maybe they're not the best fit for text classification. The low recall for class 1 (0%) suggests it's not capturing that class at all. Dimensionality reduction might have removed important features, or the model isn't capturing the text structure well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Naive Bayes model training...\n",
      "Class weights for balancing: {0: 1.0097222222222222, 1: 0.9807095642789694, 2: 1.0101431151868834}\n",
      "Training Multinomial Naive Bayes model...\n",
      "Best hyperparameters: {'alpha': 0.01, 'fit_prior': True}\n",
      "Test Accuracy: 0.8405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       600\n",
      "           1       0.73      0.93      0.82       618\n",
      "           2       0.92      0.72      0.81       600\n",
      "\n",
      "    accuracy                           0.84      1818\n",
      "   macro avg       0.86      0.84      0.84      1818\n",
      "weighted avg       0.86      0.84      0.84      1818\n",
      "\n",
      "Prediction class distribution:\n",
      "Class 0: 569 predictions\n",
      "Class 1: 781 predictions\n",
      "Class 2: 468 predictions\n",
      "Naive Bayes model saved at: d:\\Documents\\CODE\\HCMUT\\Machine Learning Assignment\\models\\trained\\naive_bayes_model.pkl\n"
     ]
    }
   ],
   "source": [
    "nb_model = trainer.train_naive_bayes(preprocessor=preprocessor)\n",
    "# nb_pred = predictor.predict_naive_bayes(X_test_vec)\n",
    "# evaluate_model(y_test, nb_pred, \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes model has an accuracy of 84.05%. That's pretty good. The classification report shows high precision for class 0 (92%) and 2 (92%), but lower for class 1 (73%). This makes sense because Naive Bayes assumes feature independence, which might not hold here, leading to some misclassifications, especially in the neutral class (1). The model might be better at detecting clear positive or negative sentiments but struggles with neutral ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RNN model training...\n",
      "Class weights for balancing: {0: 1.0097222222222222, 1: 0.9807095642789694, 2: 1.0101431151868834}\n",
      "Epoch 1/20\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 31ms/step - accuracy: 0.5767 - loss: 0.8855 - val_accuracy: 0.8535 - val_loss: 0.4336 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.8627 - loss: 0.3978 - val_accuracy: 0.8638 - val_loss: 0.3760 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.9158 - loss: 0.2645 - val_accuracy: 0.8673 - val_loss: 0.3687 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.9384 - loss: 0.2150 - val_accuracy: 0.8631 - val_loss: 0.4282 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.9514 - loss: 0.1511 - val_accuracy: 0.8624 - val_loss: 0.4429 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.9654 - loss: 0.1152 - val_accuracy: 0.8618 - val_loss: 0.5039 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.9744 - loss: 0.0912 - val_accuracy: 0.8645 - val_loss: 0.6706 - learning_rate: 5.0000e-04\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n",
      "Test Accuracy: 0.8537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       600\n",
      "           1       0.77      0.89      0.83       618\n",
      "           2       0.88      0.79      0.83       600\n",
      "\n",
      "    accuracy                           0.85      1818\n",
      "   macro avg       0.86      0.85      0.85      1818\n",
      "weighted avg       0.86      0.85      0.85      1818\n",
      "\n",
      "Prediction class distribution:\n",
      "Class 0: 559 predictions\n",
      "Class 1: 719 predictions\n",
      "Class 2: 540 predictions\n"
     ]
    }
   ],
   "source": [
    "nn_model = trainer.train_neural_network(batch_size=8)\n",
    "# nn_pred = predictor.predict_neural_network()\n",
    "# evaluate_model(y_test, nn_pred, \"Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network (RNN) achieved 85.37% accuracy, which is the highest. The confusion matrix indicates it's good across all classes but slightly weaker in class 2 (positive). RNNs are good at capturing sequential data in text, so they can understand context better, leading to higher accuracy. The balanced precision and recall suggest it generalizes well, though there's room for improvement in class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bayesian Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 236/1000000 [07:29<528:30:25,  1.90s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bayesian Network Performance ===\n",
      "Accuracy: 0.7514\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.73      0.83       600\n",
      "           1       0.61      0.90      0.73       618\n",
      "           2       0.83      0.62      0.71       600\n",
      "\n",
      "    accuracy                           0.75      1818\n",
      "   macro avg       0.80      0.75      0.75      1818\n",
      "weighted avg       0.80      0.75      0.75      1818\n",
      "\n",
      "Confusion Matrix:\n",
      "[[439 134  27]\n",
      " [ 15 554  49]\n",
      " [  7 220 373]]\n"
     ]
    }
   ],
   "source": [
    "bayesian_model = trainer.train_bayesian_network(X_train, y_train)\n",
    "bayesian_pred = predictor.predict_bayesian_network(X_test)\n",
    "evaluate_model(y_test, bayesian_pred, \"Bayesian Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Decision Tree Performance ===\n",
      "Accuracy: 0.8179\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88       600\n",
      "           1       0.69      0.94      0.80       618\n",
      "           2       0.90      0.68      0.78       600\n",
      "\n",
      "    accuracy                           0.82      1818\n",
      "   macro avg       0.85      0.82      0.82      1818\n",
      "weighted avg       0.84      0.82      0.82      1818\n",
      "\n",
      "Confusion Matrix:\n",
      "[[500  76  24]\n",
      " [ 20 578  20]\n",
      " [ 10 181 409]]\n"
     ]
    }
   ],
   "source": [
    "dt_model = trainer.train_decision_tree(X_train_vec, y_train)\n",
    "dt_pred = predictor.predict_decision_tree(X_test_vec, trainer)\n",
    "evaluate_model(y_test, dt_pred, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree model has 81.79% accuracy. It's performing well, especially in class 0 (negative) with high precision (94%). However, class 1 (neutral) has lower precision (69%), indicating it's often misclassified. Decision Trees might overfit or struggle with imbalanced data, but their interpretability is a plus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "This notebook has demonstrated the training and evaluation of multiple classification models on text data. The evaluation metrics can be compared to determine which model performs best for this particular dataset and task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
